# Google Cloud Vision API - Kindleæœ¬OCRå®Ÿè£…ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ

## ğŸ¯ ç›®çš„
macOSç’°å¢ƒã§Kindleæœ¬ã®ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã—ã€æ§‹é€ åŒ–ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆã™ã‚‹ã‚·ã‚¹ãƒ†ãƒ ã®é–‹ç™º

## ğŸ“š L1: Cloud Vision APIæ¦‚è¦

### 1.1 ä¸»è¦æ©Ÿèƒ½ã‚«ãƒ†ã‚´ãƒª
- **ç”»åƒè§£æ**: ãƒ©ãƒ™ãƒ«æ¤œå‡ºã€é¡”èªè­˜ã€ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯è­˜åˆ¥
- **ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º**: OCRã€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ†ã‚­ã‚¹ãƒˆæ¤œå‡ºã€æ‰‹æ›¸ãèªè­˜
- **ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç®¡ç†**: æ˜ç¤ºçš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ã‚¿ã‚°ä»˜ã‘ã€ç”»åƒå±æ€§

### 1.2 ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆä½ç½®ã¥ã‘
- **ç”¨é€”**: ç”»åƒã‹ã‚‰ã®æƒ…å ±æŠ½å‡ºã¨åˆ†æ
- **ä»£æ›¿é¸æŠè‚¢**: 
  - Vertex AI (ã‚«ã‚¹ã‚¿ãƒ ãƒ“ã‚¸ãƒ§ãƒ³ãƒ¢ãƒ‡ãƒ«ç”¨)
  - ML Kit (ãƒ¢ãƒã‚¤ãƒ«ç‰¹åŒ–å®Ÿè£…ç”¨)
  - Document AI (é«˜åº¦ãªæ–‡æ›¸å‡¦ç†ç”¨)

## ğŸ“– L2: OCRæ©Ÿèƒ½è©³ç´°

### 2.1 OCRãƒ¡ã‚½ãƒƒãƒ‰ã‚¿ã‚¤ãƒ—

#### TEXT_DETECTION
- **ç”¨é€”**: ä¸€èˆ¬çš„ãªç”»åƒã‹ã‚‰ã®ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºï¼ˆçœ‹æ¿ã€ãƒ©ãƒ™ãƒ«ç­‰ï¼‰
- **ç‰¹å¾´**: ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
- **æ¨å¥¨**: ç–ãªãƒ†ã‚­ã‚¹ãƒˆã€å˜ç´”ãªãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ

#### DOCUMENT_TEXT_DETECTION
- **ç”¨é€”**: å¯†åº¦ã®é«˜ã„ãƒ†ã‚­ã‚¹ãƒˆã¨æ–‡æ›¸ç”¨ã«æœ€é©åŒ–
- **ç‰¹å¾´**: 
  - ãƒšãƒ¼ã‚¸/ãƒ–ãƒ­ãƒƒã‚¯/æ®µè½/å˜èªã®éšå±¤æ§‹é€ 
  - ã‚ˆã‚Šè©³ç´°ãªä½ç½®æƒ…å ±
  - èª­ã¿é †åºã®ä¿æŒ
- **æ¨å¥¨**: **Kindleæœ¬ã®OCRã«ã¯ã“ã¡ã‚‰ã‚’ä½¿ç”¨**

### 2.2 ã‚µãƒãƒ¼ãƒˆå½¢å¼ã¨åˆ¶é™

#### ç”»åƒãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
```yaml
supported_formats:
  - JPEG
  - PNG (PNG8, PNG24)
  - GIF (æœ€åˆã®ãƒ•ãƒ¬ãƒ¼ãƒ ã®ã¿)
  - BMP
  - WEBP
  - RAW
  - ICO
  - PDF (Cloud StorageçµŒç”±)
  - TIFF (Cloud StorageçµŒç”±)
```

#### ã‚µã‚¤ã‚ºåˆ¶é™
```yaml
limits:
  max_file_size: 20MB
  max_json_request: 10MB
  max_pixels: 75,000,000 (75M)
  pdf_max_pages: 2000
  recommended_resolution:
    ocr: "1024 x 768"
    optimal: "1600 x 1200"
```

## ğŸ”§ L3: å®Ÿè£…è©³ç´°

### 3.1 APIå‘¼ã³å‡ºã—æ–¹æ³•

#### Pythonå®Ÿè£…ä¾‹
```python
from google.cloud import vision
import io

def extract_text_from_image(image_path):
    """Kindleæœ¬ã®ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
    client = vision.ImageAnnotatorClient()
    
    with io.open(image_path, 'rb') as image_file:
        content = image_file.read()
    
    image = vision.Image(content=content)
    
    # DOCUMENT_TEXT_DETECTIONã‚’ä½¿ç”¨ï¼ˆæ›¸ç±ã«æœ€é©ï¼‰
    response = client.document_text_detection(
        image=image,
        image_context={'language_hints': ['ja']}  # æ—¥æœ¬èªãƒ’ãƒ³ãƒˆ
    )
    
    return response.full_text_annotation
```

### 3.2 ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ§‹é€ 

```json
{
  "fullTextAnnotation": {
    "pages": [{
      "blocks": [{
        "paragraphs": [{
          "words": [{
            "symbols": [{
              "text": "æ–‡",
              "boundingBox": {...},
              "confidence": 0.99
            }]
          }]
        }]
      }]
    }],
    "text": "å®Œå…¨ãªãƒ†ã‚­ã‚¹ãƒˆå†…å®¹"
  },
  "textAnnotations": [{
    "description": "æ¤œå‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ",
    "boundingPoly": {
      "vertices": [
        {"x": 0, "y": 0},
        {"x": 100, "y": 0},
        {"x": 100, "y": 50},
        {"x": 0, "y": 50}
      ]
    }
  }]
}
```

### 3.3 è¨€èªã‚µãƒãƒ¼ãƒˆ

#### æ—¥æœ¬èªOCRè¨­å®š
```python
image_context = {
    'language_hints': ['ja'],  # æ—¥æœ¬èªå„ªå…ˆ
    # ã¾ãŸã¯è‡ªå‹•æ¤œå‡ºï¼ˆæ¨å¥¨ï¼‰
    'language_hints': []
}
```

#### æ‰‹æ›¸ãèªè­˜
```python
# æ‰‹æ›¸ãæ—¥æœ¬èªã®å ´åˆ
image_context = {
    'language_hints': ['ja-t-i0-handwrit']
}
```

## ğŸ’° L4: å®Ÿè£…ä»•æ§˜ã¨ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

### 4.1 æ–™é‡‘ä½“ç³»

```yaml
pricing:
  free_tier: 
    units: 1000
    period: monthly
  
  document_text_detection:
    tier1:
      range: "1,001 - 5,000,000"
      price: "$1.50 per 1,000 units"
    tier2:
      range: "5,000,001+"
      price: "$0.60 per 1,000 units"
```

### 4.2 Kindleæœ¬OCRæœ€é©åŒ–è¨­å®š

#### æ¨å¥¨å‰å‡¦ç†
```python
def preprocess_kindle_screenshot(image_path):
    """Kindleæœ¬ã®ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆå‰å‡¦ç†"""
    from PIL import Image
    
    img = Image.open(image_path)
    
    # æ¨å¥¨è¨­å®š
    optimal_width = 1600
    optimal_height = 1200
    
    # ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã‚’ä¿æŒã—ã¦ãƒªã‚µã‚¤ã‚º
    img.thumbnail((optimal_width, optimal_height), Image.LANCZOS)
    
    # ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«å¤‰æ›ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    # img = img.convert('L')
    
    # ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆèª¿æ•´
    from PIL import ImageEnhance
    enhancer = ImageEnhance.Contrast(img)
    img = enhancer.enhance(1.2)
    
    return img
```

### 4.3 ãƒãƒƒãƒå‡¦ç†å®Ÿè£…

```python
def batch_process_kindle_pages(image_paths, batch_size=10):
    """è¤‡æ•°ãƒšãƒ¼ã‚¸ã®ä¸€æ‹¬å‡¦ç†"""
    from google.cloud import vision
    
    client = vision.ImageAnnotatorClient()
    results = []
    
    for i in range(0, len(image_paths), batch_size):
        batch = image_paths[i:i+batch_size]
        requests = []
        
        for path in batch:
            with open(path, 'rb') as f:
                content = f.read()
            
            image = vision.Image(content=content)
            requests.append({
                'image': image,
                'features': [{
                    'type': vision.Feature.Type.DOCUMENT_TEXT_DETECTION
                }]
            })
        
        responses = client.batch_annotate_images(requests=requests)
        results.extend(responses.responses)
    
    return results
```

### 4.4 macOSçµ±åˆã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

#### ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆè‡ªå‹•å–å¾—
```python
import subprocess
import time

def capture_kindle_page():
    """macOSã§Kindleã‚¢ãƒ—ãƒªã®ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã‚’å–å¾—"""
    # ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚­ãƒ£ãƒ—ãƒãƒ£ã‚³ãƒãƒ³ãƒ‰
    subprocess.run([
        'screencapture', 
        '-x',  # éŸ³ã‚’æ¶ˆã™
        '-R', '100,100,1000,1400',  # é ˜åŸŸæŒ‡å®š
        'kindle_page.png'
    ])
    time.sleep(0.5)
    return 'kindle_page.png'
```

#### Automatoré€£æº
```applescript
-- AppleScript for Kindle page turning and capture
tell application "Kindle"
    activate
    delay 1
    tell application "System Events"
        key code 124 -- Right arrow for next page
    end tell
end tell
do shell script "screencapture -x ~/Desktop/kindle_page.png"
```

### 4.5 ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

```python
def safe_ocr_extraction(image_path, max_retries=3):
    """ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä»˜ãOCR"""
    from google.cloud import vision
    from google.api_core import retry
    import time
    
    client = vision.ImageAnnotatorClient()
    
    @retry.Retry(deadline=30)
    def extract_with_retry():
        try:
            with open(image_path, 'rb') as f:
                image = vision.Image(content=f.read())
            
            response = client.document_text_detection(image=image)
            
            if response.error.message:
                raise Exception(f'API Error: {response.error.message}')
            
            return response
            
        except Exception as e:
            print(f"Error: {e}")
            time.sleep(1)
            raise
    
    return extract_with_retry()
```

### 4.6 æ§‹é€ åŒ–å‡ºåŠ›ç”Ÿæˆ

```python
def generate_structured_context(ocr_response):
    """OCRçµæœã‹ã‚‰æ§‹é€ åŒ–ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ"""
    structured_data = {
        'metadata': {
            'source': 'kindle_screenshot',
            'extraction_date': datetime.now().isoformat(),
            'confidence': calculate_average_confidence(ocr_response)
        },
        'content': {
            'full_text': ocr_response.full_text_annotation.text,
            'pages': []
        }
    }
    
    for page in ocr_response.full_text_annotation.pages:
        page_data = {
            'blocks': [],
            'paragraphs': []
        }
        
        for block in page.blocks:
            block_text = extract_block_text(block)
            page_data['blocks'].append({
                'text': block_text,
                'confidence': block.confidence
            })
        
        structured_data['content']['pages'].append(page_data)
    
    return structured_data
```

## ğŸš€ å®Ÿè£…ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

### å¿…é ˆå®Ÿè£…é …ç›®
- [ ] Google Cloud ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ä½œæˆ
- [ ] Vision API ã®æœ‰åŠ¹åŒ–
- [ ] ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚­ãƒ¼ã®ç”Ÿæˆ
- [ ] Python ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
- [ ] DOCUMENT_TEXT_DETECTION ã®å®Ÿè£…
- [ ] æ—¥æœ¬èªè¨€èªãƒ’ãƒ³ãƒˆã®è¨­å®š
- [ ] ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
- [ ] ãƒãƒƒãƒå‡¦ç†ã®å®Ÿè£…

### æ¨å¥¨å®Ÿè£…é …ç›®
- [ ] ç”»åƒå‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
- [ ] macOS Automator ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
- [ ] ãƒšãƒ¼ã‚¸è‡ªå‹•ã‚ãã‚Šæ©Ÿèƒ½
- [ ] æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿å‡ºåŠ›
- [ ] ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼è¡¨ç¤º
- [ ] OCRçµæœã®ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°
- [ ] é‡è¤‡ãƒ†ã‚­ã‚¹ãƒˆé™¤å»
- [ ] ç« ãƒ»ç¯€ã®è‡ªå‹•èªè­˜

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–
- [ ] ä¸¦åˆ—å‡¦ç†ã®å®Ÿè£…
- [ ] ç”»åƒåœ§ç¸®ã®æœ€é©åŒ–
- [ ] ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®ç›£è¦–
- [ ] APIå‘¼ã³å‡ºã—ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã®å®Ÿè£…

## ğŸ“Š æœŸå¾…ã•ã‚Œã‚‹æˆæœ

### ã‚·ã‚¹ãƒ†ãƒ è¦ä»¶
```yaml
performance:
  processing_speed: "< 2ç§’/ãƒšãƒ¼ã‚¸"
  accuracy: "> 95% (æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆ)"
  batch_size: "10-20ãƒšãƒ¼ã‚¸/ãƒãƒƒãƒ"
  
cost:
  estimated_monthly: "< $10 (1000ãƒšãƒ¼ã‚¸/æœˆ)"
  free_tier_coverage: "æœ€åˆã®1000ãƒ¦ãƒ‹ãƒƒãƒˆ"
  
quality:
  text_preservation: "99%"
  layout_detection: "ãƒ–ãƒ­ãƒƒã‚¯ãƒ»æ®µè½ãƒ¬ãƒ™ãƒ«"
  language_detection: "è‡ªå‹•"
```

## ğŸ”— é–¢é€£ãƒªã‚½ãƒ¼ã‚¹

- [Cloud Vision API ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://cloud.google.com/vision/docs)
- [OCRæ©Ÿèƒ½è©³ç´°](https://cloud.google.com/vision/docs/ocr)
- [æ–™é‡‘è¨ˆç®—ãƒ„ãƒ¼ãƒ«](https://cloud.google.com/vision/pricing)
- [ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãƒ©ã‚¤ãƒ–ãƒ©ãƒª](https://cloud.google.com/vision/docs/libraries)
- [Document AI (é«˜åº¦ãªå‡¦ç†)](https://cloud.google.com/document-ai)

---

*ã“ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¯ã€macOSç’°å¢ƒã§Kindleæœ¬ã®OCRå‡¦ç†ã‚’å®Ÿè£…ã™ã‚‹ãŸã‚ã®åŒ…æ‹¬çš„ãªã‚¬ã‚¤ãƒ‰ã§ã™ã€‚*
*æœ€çµ‚æ›´æ–°: 2025-08-24*